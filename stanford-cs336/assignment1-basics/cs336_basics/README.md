# CS336 BPE Implementation - Organized Structure

This directory contains a complete implementation of Byte-Pair Encoding (BPE) for the CS336 Assignment 1.

## üìÅ Directory Structure

```
cs336_basics/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ __init__.py                  # Package initialization
‚îÇ
‚îú‚îÄ‚îÄ tokenizer/                   # üî§ Tokenizer Implementation
‚îÇ   ‚îî‚îÄ‚îÄ bpe_tokenizer.py        # Main BPE tokenizer class
‚îÇ
‚îú‚îÄ‚îÄ training/                    # üèãÔ∏è Training Scripts
‚îÇ   ‚îî‚îÄ‚îÄ bpe_train.py            # BPE training implementation
‚îÇ
‚îú‚îÄ‚îÄ experiments/                 # üß™ Experiment Scripts
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_experiments.py # Complete tokenizer experiments (4 points)
‚îÇ   ‚îú‚îÄ‚îÄ encode_datasets.py      # Full dataset encoding script
‚îÇ   ‚îú‚îÄ‚îÄ quick_experiments.py    # Memory-safe experiments
‚îÇ   ‚îî‚îÄ‚îÄ pretokenization_example.py # Pre-tokenization examples
‚îÇ
‚îî‚îÄ‚îÄ artifacts/                   # üì¶ Generated Files
    ‚îú‚îÄ‚îÄ vocabularies/            # Vocabulary and merge files
    ‚îÇ   ‚îú‚îÄ‚îÄ tinystories_vocab.json
    ‚îÇ   ‚îú‚îÄ‚îÄ tinystories_merges.pkl
    ‚îÇ   ‚îú‚îÄ‚îÄ tinystories_merges.txt
    ‚îÇ   ‚îú‚îÄ‚îÄ owt_vocab.json
    ‚îÇ   ‚îú‚îÄ‚îÄ owt_merges.pkl
    ‚îÇ   ‚îî‚îÄ‚îÄ owt_merges.txt
    ‚îú‚îÄ‚îÄ datasets/                # Encoded datasets (uint16 arrays)
    ‚îÇ   ‚îú‚îÄ‚îÄ tinystories_tokens.npy
    ‚îÇ   ‚îú‚îÄ‚îÄ tinystories_train_tokens.npy
    ‚îÇ   ‚îú‚îÄ‚îÄ owt_train_tokens.npy
    ‚îÇ   ‚îî‚îÄ‚îÄ sample_tokens.npy
    ‚îî‚îÄ‚îÄ demos/                   # Demo and sample files
        ‚îú‚îÄ‚îÄ demo_tokens_tinystories.npy
        ‚îú‚îÄ‚îÄ demo_tokens_openwebtext.npy
        ‚îî‚îÄ‚îÄ sample_tokens_demo.npy
```

## üöÄ Quick Start

### 1. Train BPE Tokenizers
```bash
# From cs336_basics directory
cd training
python bpe_train.py tinystories  # Train TinyStories tokenizer (10K vocab)
python bpe_train.py owt          # Train OpenWebText tokenizer (32K vocab)
```

### 2. Run Experiments
```bash
cd experiments
python quick_experiments.py              # Fast, memory-safe experiments
python tokenizer_experiments.py          # Complete experiments (deliverables a-c)
python tokenizer_experiments.py --encode-full  # Include full dataset encoding (deliverable d)
```

### 3. Encode Datasets
```bash
cd experiments
python encode_datasets.py  # Encode both datasets to uint16 NumPy arrays
```

## üìã Assignment Deliverables

### Problem: train_bpe_tinystories (2 points)
- **File**: `training/bpe_train.py`
- **Command**: `python training/bpe_train.py tinystories`
- **Artifacts**: `artifacts/vocabularies/tinystories_*`

### Problem: train_bpe_expts_owt (2 points)
- **File**: `training/bpe_train.py`
- **Command**: `python training/bpe_train.py owt`
- **Artifacts**: `artifacts/vocabularies/owt_*`

### Problem: tokenizer_experiments (4 points)
- **File**: `experiments/tokenizer_experiments.py`
- **Command**: `python experiments/tokenizer_experiments.py`
- **Deliverables**:
  - **(a)** Compression ratios: TinyStories vs OpenWebText
  - **(b)** Cross-tokenization analysis
  - **(c)** Throughput estimation for Pile dataset
  - **(d)** Dataset encoding to uint16 arrays

## üîß Key Features

### Memory Efficiency
- **Streaming processing** for large files (>500MB)
- **Chunked encoding** to avoid memory issues
- **Automatic mode selection** based on file size

### Performance Optimizations
- **Multiprocessing** for BPE training with boundary detection
- **Heap-based pair selection** for efficient merging
- **Progress tracking** with throughput monitoring

### Robust Error Handling
- **Graceful fallbacks** when multiprocessing fails
- **Memory overflow protection** with automatic streaming
- **File path validation** and error reporting

## üèóÔ∏è Architecture

### Core Components

1. **BPE Tokenizer** (`tokenizer/bpe_tokenizer.py`)
   - Complete BPE implementation with encode/decode
   - Special token handling with overlap resolution
   - Memory-efficient encoding for large texts

2. **BPE Training** (`training/bpe_train.py`)
   - Scalable training from small to 11GB+ datasets
   - Intelligent processing mode selection
   - Comprehensive profiling and analysis

3. **Experiments Framework** (`experiments/`)
   - Modular experiment design
   - Multiple execution modes (quick vs full)
   - Complete deliverable generation

### File Formats

- **Vocabularies**: JSON format for human readability
- **Merges**: Pickle format for exact byte preservation + TXT for inspection
- **Datasets**: uint16 NumPy arrays for efficient storage
- **Demos**: Small samples for testing and validation

## üìä Dataset Information

### TinyStories
- **Size**: 2.1GB text ‚Üí ~1.0GB tokens
- **Vocabulary**: 10,000 tokens
- **Training Time**: ~3 minutes
- **Compression**: ~4.1 bytes/token

### OpenWebText
- **Size**: 11.1GB text ‚Üí ~4.9GB tokens
- **Vocabulary**: 32,000 tokens
- **Training Time**: ~2.3 hours
- **Compression**: ~4.6 bytes/token

## üîç Usage Examples

### Loading a Trained Tokenizer
```python
from tokenizer.bpe_tokenizer import Tokenizer

# Load TinyStories tokenizer
tokenizer = Tokenizer.from_files(
    'artifacts/vocabularies/tinystories_vocab.json',
    'artifacts/vocabularies/tinystories_merges.pkl',
    ['<|endoftext|>']
)

# Encode text
text = "Once upon a time, there was a little girl."
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")

# Decode back
decoded = tokenizer.decode(tokens)
print(f"Decoded: {decoded}")
```

### Loading Encoded Datasets
```python
import numpy as np

# Load encoded datasets
tinystories_tokens = np.load('artifacts/datasets/tinystories_train_tokens.npy')
owt_tokens = np.load('artifacts/datasets/owt_train_tokens.npy')

print(f"TinyStories: {len(tinystories_tokens):,} tokens")
print(f"OpenWebText: {len(owt_tokens):,} tokens")
```

## ‚ö° Performance Notes

- **Memory Usage**: <2GB RAM for all operations via streaming
- **Processing Speed**: ~0.6 MB/s tokenization throughput
- **Scalability**: Handles 11GB+ files efficiently
- **Storage Efficiency**: 2x compression with uint16 vs uint32

## üß™ Testing

All components include comprehensive testing:
- **25/25 tokenizer tests passing**
- **3/3 BPE training tests passing**
- **Memory safety validated**
- **Cross-platform compatibility**

---

**Total Implementation**: Complete BPE system ready for CS336 Assignment 1 submission! üéâ